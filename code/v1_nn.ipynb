{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76d8136f-94eb-4308-922f-778c6d933ca9",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "972b5d24-0264-46bc-aef8-be403e577db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import init_logger, timer, fix_seed, df_info, reduce_memory_usage\n",
    "import random\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 150)\n",
    "pd.set_option('display.max_columns', 150)\n",
    "import numpy as np\n",
    "import category_encoders as ce\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas(desc=\"Processing:\")\n",
    "\n",
    "fix_seed()\n",
    "logger = init_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "543c8c49-ac3f-459b-9e1a-14abb9068e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dcf465-00a9-4af5-a0c3-10a8ab81d29f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4abd8e9-2191-4f51-bd68-9ffd1f4771cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import category_encoders as ce\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def load_data():\n",
    "    df_train = pd.read_csv(\"../input/train.csv\")\n",
    "    df_test = pd.read_csv(\"../input/test.csv\")\n",
    "    df = pd.concat([df_train, df_test]).reset_index(drop=True)\n",
    "    \n",
    "    # missing value\n",
    "    df['Fare'].fillna(df.query('Pclass==3 & Embarked==\"S\"')['Fare'].median(), inplace=True)\n",
    "    df[\"Age\"].fillna(df[\"Age\"].mean(), inplace=True)\n",
    "    \n",
    "    # ordinal encoding\n",
    "    df[\"Sex\"] = df[\"Sex\"].map({\"male\": 0, \"female\": 1})\n",
    "    \n",
    "    # one hot encoding\n",
    "    ohe_columns = [\n",
    "        \"Pclass\",\n",
    "        \"Embarked\"\n",
    "    ]\n",
    "    ohe = ce.OneHotEncoder(cols=ohe_columns, handle_unknown='impute')\n",
    "    df = ohe.fit_transform(df)\n",
    "    \n",
    "    # scaling\n",
    "    sc_columns = [\n",
    "        \"Age\",\n",
    "        \"Fare\"\n",
    "    ]\n",
    "    sc = StandardScaler()\n",
    "    df[sc_columns] = sc.fit_transform(df[sc_columns])\n",
    "    \n",
    "    # extract\n",
    "    df.drop([\n",
    "        \"Name\",\n",
    "        \"Ticket\",\n",
    "        \"Cabin\"\n",
    "    ], axis=1, inplace=True)\n",
    "\n",
    "    \n",
    "    df_train, df_test = df[:len(df_train)], df[len(df_train):]\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bf1ac82-2cf3-4ec3-8b99-0e554d1932b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/01/02 15:37:33 45 [INFO] [read csv] start.\n",
      "2022/01/02 15:37:33 47 [INFO] [read csv] done in 0.035 seconds.\n"
     ]
    }
   ],
   "source": [
    "with timer(\"read csv\", logger):\n",
    "    df_train, df_test = load_data()\n",
    "\n",
    "df_train = reduce_memory_usage(df_train)\n",
    "df_test = reduce_memory_usage(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e948dca9-f964-4a48-a9c4-f74413c457ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop([\"PassengerId\", \"Survived\"], axis=1)\n",
    "y_train = df_train[\"Survived\"]\n",
    "X_test = df_test.drop([\"PassengerId\", \"Survived\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0de4f451-26a0-4982-9325-5701c804fe71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked_1</th>\n",
       "      <th>Embarked_2</th>\n",
       "      <th>Embarked_3</th>\n",
       "      <th>Embarked_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.611816</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.503418</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.630371</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.734863</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.301270</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.490234</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.397461</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.383301</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.397461</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.487793</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass_1  Pclass_2  Pclass_3  Sex       Age  SibSp  Parch      Fare  \\\n",
       "0         1         0         0    0 -0.611816      1      0 -0.503418   \n",
       "1         0         1         0    1  0.630371      1      0  0.734863   \n",
       "2         1         0         0    1 -0.301270      0      0 -0.490234   \n",
       "3         0         1         0    1  0.397461      1      0  0.383301   \n",
       "4         1         0         0    0  0.397461      0      0 -0.487793   \n",
       "\n",
       "   Embarked_1  Embarked_2  Embarked_3  Embarked_4  \n",
       "0           1           0           0           0  \n",
       "1           0           1           0           0  \n",
       "2           1           0           0           0  \n",
       "3           1           0           0           0  \n",
       "4           1           0           0           0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c607f2-84f6-4ec0-8fb3-8800cb738cf6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20d74fee-5ecd-48b9-a51f-df20c97b935c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_nn import ModelNN\n",
    "from runner import Runner\n",
    "from keras.callbacks import EarlyStopping\n",
    "from hyperopt import hp, fmin, tpe, space_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd7219ce-2a81-4f57-9d60-2f3f21beafe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = Runner(PROJECT_NAME, ModelNN, cv=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "823ecd1e-fb26-42a0-a27d-0a932ee91eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"units_list\": [12, 8, 4],\n",
    "    \"dropout\": 0.2,\n",
    "    \"num_classes\": 2\n",
    "}\n",
    "\n",
    "# space = {\n",
    "#     \"layers\": hp.uniformint(\"layer\", 3, 7),\n",
    "#     \"dropout\": hp.uniform(\"dropout\", 0.1, 0.4),\n",
    "#     \"units\": hp.choice(\"units\", [4, 8, 12, 16])\n",
    "# }\n",
    "\n",
    "train_params = {\n",
    "    \"epochs\": 5000,\n",
    "    \"batch_size\": 32,\n",
    "    \"verbose\": 1,\n",
    "    \"callbacks\": [EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=30, verbose=1)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee343e70-78ba-409a-818e-4902b175c3ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def objective(args):\n",
    "    logger.info(args)\n",
    "    runner.train(X_train, y_train, args, train_params)\n",
    "    return runner.get_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d0261a7-4e64-445c-b503-5cfb13ff5fcb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                     | 0/2 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/01/01 11:37:07 2 [INFO] {'dropout': 0.35533811619227473, 'layers': 6, 'units': 12}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Max\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-01 11:37:07.285715: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-01-01 11:37:07.285887: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2022-01-01 11:37:07.406809: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-01-01 11:37:07.787040: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-01-01 11:37:09.189156: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00221: early stopping                                                     \n",
      " 50%|█████     | 1/2 [01:28<01:28, 88.49s/trial, best loss: 0.37501025199890137]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/01/01 11:38:35 2 [INFO] {'dropout': 0.14697523300939222, 'layers': 5, 'units': 4}\n",
      "2022-01-01 11:38:36.206935: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-01-01 11:38:37.315807: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00220: early stopping                                                     \n",
      "100%|██████████| 2/2 [02:43<00:00, 81.89s/trial, best loss: 0.37501025199890137]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/01/01 11:39:51 3 [INFO] best params: {'dropout': 0.35533811619227473, 'layers': 6, 'units': 12}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=2)\n",
    "best_params = space_eval(space, best)\n",
    "logger.info(\"best params: {}\".format(best_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa4545a-8cb4-40cf-8eea-1478e43f2f2b",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43cc9a29-ed85-4167-8919-06a59931fd01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/01/02 15:38:53 45 [INFO] [train] start.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-02 15:38:53.896372: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 1s 24ms/step - loss: 0.8416 - accuracy: 0.4747 - val_loss: 0.7696 - val_accuracy: 0.5196\n",
      "Epoch 2/5000\n",
      " 5/23 [=====>........................] - ETA: 0s - loss: 0.7460 - accuracy: 0.5312"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-02 15:38:54.582021: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 17ms/step - loss: 0.7291 - accuracy: 0.5674 - val_loss: 0.6882 - val_accuracy: 0.5419\n",
      "Epoch 3/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.6714 - accuracy: 0.6124 - val_loss: 0.6345 - val_accuracy: 0.6313\n",
      "Epoch 4/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.6491 - accuracy: 0.6489 - val_loss: 0.5930 - val_accuracy: 0.7039\n",
      "Epoch 5/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.6041 - accuracy: 0.6713 - val_loss: 0.5666 - val_accuracy: 0.7430\n",
      "Epoch 6/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.6061 - accuracy: 0.6770 - val_loss: 0.5428 - val_accuracy: 0.7598\n",
      "Epoch 7/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.5887 - accuracy: 0.6826 - val_loss: 0.5241 - val_accuracy: 0.7709\n",
      "Epoch 8/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.5460 - accuracy: 0.7374 - val_loss: 0.5061 - val_accuracy: 0.7765\n",
      "Epoch 9/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.5522 - accuracy: 0.7360 - val_loss: 0.4901 - val_accuracy: 0.7877\n",
      "Epoch 10/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.5199 - accuracy: 0.7528 - val_loss: 0.4798 - val_accuracy: 0.7877\n",
      "Epoch 11/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.5180 - accuracy: 0.7612 - val_loss: 0.4665 - val_accuracy: 0.8045\n",
      "Epoch 12/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.5503 - accuracy: 0.7317 - val_loss: 0.4613 - val_accuracy: 0.8101\n",
      "Epoch 13/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.5113 - accuracy: 0.7823 - val_loss: 0.4540 - val_accuracy: 0.8045\n",
      "Epoch 14/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.5220 - accuracy: 0.7598 - val_loss: 0.4479 - val_accuracy: 0.8101\n",
      "Epoch 15/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.5120 - accuracy: 0.7528 - val_loss: 0.4436 - val_accuracy: 0.8045\n",
      "Epoch 16/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.5221 - accuracy: 0.7584 - val_loss: 0.4411 - val_accuracy: 0.8156\n",
      "Epoch 17/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.5079 - accuracy: 0.7753 - val_loss: 0.4406 - val_accuracy: 0.8212\n",
      "Epoch 18/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.5271 - accuracy: 0.7458 - val_loss: 0.4358 - val_accuracy: 0.8045\n",
      "Epoch 19/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.5089 - accuracy: 0.7767 - val_loss: 0.4376 - val_accuracy: 0.8045\n",
      "Epoch 20/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.5112 - accuracy: 0.7711 - val_loss: 0.4362 - val_accuracy: 0.8212\n",
      "Epoch 21/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4949 - accuracy: 0.7640 - val_loss: 0.4346 - val_accuracy: 0.8212\n",
      "Epoch 22/5000\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.5199 - accuracy: 0.7486 - val_loss: 0.4328 - val_accuracy: 0.8212\n",
      "Epoch 23/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.5078 - accuracy: 0.7697 - val_loss: 0.4325 - val_accuracy: 0.8156\n",
      "Epoch 24/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.5018 - accuracy: 0.7837 - val_loss: 0.4305 - val_accuracy: 0.8212\n",
      "Epoch 25/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4912 - accuracy: 0.7739 - val_loss: 0.4306 - val_accuracy: 0.8156\n",
      "Epoch 26/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.5034 - accuracy: 0.7767 - val_loss: 0.4325 - val_accuracy: 0.8101\n",
      "Epoch 27/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.5040 - accuracy: 0.7697 - val_loss: 0.4315 - val_accuracy: 0.8101\n",
      "Epoch 28/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4951 - accuracy: 0.7823 - val_loss: 0.4315 - val_accuracy: 0.8101\n",
      "Epoch 29/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4988 - accuracy: 0.7795 - val_loss: 0.4321 - val_accuracy: 0.8156\n",
      "Epoch 30/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.5001 - accuracy: 0.7795 - val_loss: 0.4293 - val_accuracy: 0.8268\n",
      "Epoch 31/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4946 - accuracy: 0.7851 - val_loss: 0.4286 - val_accuracy: 0.8212\n",
      "Epoch 32/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4917 - accuracy: 0.7823 - val_loss: 0.4268 - val_accuracy: 0.8212\n",
      "Epoch 33/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4917 - accuracy: 0.7865 - val_loss: 0.4271 - val_accuracy: 0.8156\n",
      "Epoch 34/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4979 - accuracy: 0.7753 - val_loss: 0.4263 - val_accuracy: 0.8101\n",
      "Epoch 35/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4819 - accuracy: 0.7781 - val_loss: 0.4252 - val_accuracy: 0.8156\n",
      "Epoch 36/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4783 - accuracy: 0.7823 - val_loss: 0.4261 - val_accuracy: 0.8101\n",
      "Epoch 37/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4745 - accuracy: 0.7935 - val_loss: 0.4253 - val_accuracy: 0.8101\n",
      "Epoch 38/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4666 - accuracy: 0.7893 - val_loss: 0.4240 - val_accuracy: 0.8156\n",
      "Epoch 39/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4784 - accuracy: 0.7978 - val_loss: 0.4237 - val_accuracy: 0.8101\n",
      "Epoch 40/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4962 - accuracy: 0.7739 - val_loss: 0.4232 - val_accuracy: 0.8156\n",
      "Epoch 41/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4799 - accuracy: 0.7949 - val_loss: 0.4227 - val_accuracy: 0.8101\n",
      "Epoch 42/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4988 - accuracy: 0.7879 - val_loss: 0.4227 - val_accuracy: 0.8101\n",
      "Epoch 43/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4823 - accuracy: 0.7921 - val_loss: 0.4221 - val_accuracy: 0.8156\n",
      "Epoch 44/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4865 - accuracy: 0.7837 - val_loss: 0.4210 - val_accuracy: 0.8156\n",
      "Epoch 45/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4942 - accuracy: 0.7795 - val_loss: 0.4200 - val_accuracy: 0.8156\n",
      "Epoch 46/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4941 - accuracy: 0.7837 - val_loss: 0.4208 - val_accuracy: 0.8324\n",
      "Epoch 47/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4675 - accuracy: 0.7921 - val_loss: 0.4206 - val_accuracy: 0.8324\n",
      "Epoch 48/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4908 - accuracy: 0.7781 - val_loss: 0.4207 - val_accuracy: 0.8324\n",
      "Epoch 49/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4776 - accuracy: 0.7935 - val_loss: 0.4202 - val_accuracy: 0.8324\n",
      "Epoch 50/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4788 - accuracy: 0.7921 - val_loss: 0.4193 - val_accuracy: 0.8324\n",
      "Epoch 51/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4886 - accuracy: 0.7781 - val_loss: 0.4181 - val_accuracy: 0.8324\n",
      "Epoch 52/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4875 - accuracy: 0.7823 - val_loss: 0.4188 - val_accuracy: 0.8268\n",
      "Epoch 53/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4819 - accuracy: 0.8020 - val_loss: 0.4192 - val_accuracy: 0.8268\n",
      "Epoch 54/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4856 - accuracy: 0.7921 - val_loss: 0.4211 - val_accuracy: 0.8268\n",
      "Epoch 55/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4718 - accuracy: 0.7893 - val_loss: 0.4191 - val_accuracy: 0.8268\n",
      "Epoch 56/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4715 - accuracy: 0.7921 - val_loss: 0.4192 - val_accuracy: 0.8268\n",
      "Epoch 57/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4772 - accuracy: 0.7907 - val_loss: 0.4194 - val_accuracy: 0.8268\n",
      "Epoch 58/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4673 - accuracy: 0.8062 - val_loss: 0.4184 - val_accuracy: 0.8324\n",
      "Epoch 59/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4807 - accuracy: 0.7837 - val_loss: 0.4173 - val_accuracy: 0.8324\n",
      "Epoch 60/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4683 - accuracy: 0.8104 - val_loss: 0.4178 - val_accuracy: 0.8268\n",
      "Epoch 61/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4575 - accuracy: 0.8160 - val_loss: 0.4154 - val_accuracy: 0.8212\n",
      "Epoch 62/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4645 - accuracy: 0.8076 - val_loss: 0.4133 - val_accuracy: 0.8212\n",
      "Epoch 63/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4747 - accuracy: 0.7963 - val_loss: 0.4147 - val_accuracy: 0.8212\n",
      "Epoch 64/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4704 - accuracy: 0.8048 - val_loss: 0.4144 - val_accuracy: 0.8212\n",
      "Epoch 65/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4840 - accuracy: 0.7879 - val_loss: 0.4159 - val_accuracy: 0.8268\n",
      "Epoch 66/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4738 - accuracy: 0.7949 - val_loss: 0.4165 - val_accuracy: 0.8212\n",
      "Epoch 67/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4849 - accuracy: 0.7865 - val_loss: 0.4168 - val_accuracy: 0.8268\n",
      "Epoch 68/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4867 - accuracy: 0.7809 - val_loss: 0.4149 - val_accuracy: 0.8212\n",
      "Epoch 69/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4679 - accuracy: 0.7978 - val_loss: 0.4144 - val_accuracy: 0.8212\n",
      "Epoch 70/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4634 - accuracy: 0.8020 - val_loss: 0.4137 - val_accuracy: 0.8212\n",
      "Epoch 71/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4702 - accuracy: 0.8034 - val_loss: 0.4137 - val_accuracy: 0.8268\n",
      "Epoch 72/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4663 - accuracy: 0.8104 - val_loss: 0.4134 - val_accuracy: 0.8212\n",
      "Epoch 73/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4690 - accuracy: 0.7907 - val_loss: 0.4119 - val_accuracy: 0.8156\n",
      "Epoch 74/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4546 - accuracy: 0.8048 - val_loss: 0.4133 - val_accuracy: 0.8212\n",
      "Epoch 75/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4905 - accuracy: 0.8076 - val_loss: 0.4145 - val_accuracy: 0.8156\n",
      "Epoch 76/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4716 - accuracy: 0.8006 - val_loss: 0.4133 - val_accuracy: 0.8156\n",
      "Epoch 77/5000\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.4853 - accuracy: 0.7978 - val_loss: 0.4109 - val_accuracy: 0.8212\n",
      "Epoch 78/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4778 - accuracy: 0.7893 - val_loss: 0.4117 - val_accuracy: 0.8268\n",
      "Epoch 79/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4790 - accuracy: 0.7865 - val_loss: 0.4116 - val_accuracy: 0.8268\n",
      "Epoch 80/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4632 - accuracy: 0.7992 - val_loss: 0.4132 - val_accuracy: 0.8380\n",
      "Epoch 81/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4602 - accuracy: 0.7992 - val_loss: 0.4129 - val_accuracy: 0.8324\n",
      "Epoch 82/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4719 - accuracy: 0.7879 - val_loss: 0.4161 - val_accuracy: 0.8324\n",
      "Epoch 83/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4635 - accuracy: 0.7935 - val_loss: 0.4134 - val_accuracy: 0.8380\n",
      "Epoch 84/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4738 - accuracy: 0.7963 - val_loss: 0.4126 - val_accuracy: 0.8380\n",
      "Epoch 85/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4692 - accuracy: 0.7879 - val_loss: 0.4115 - val_accuracy: 0.8324\n",
      "Epoch 86/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4649 - accuracy: 0.8020 - val_loss: 0.4104 - val_accuracy: 0.8324\n",
      "Epoch 87/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4776 - accuracy: 0.8020 - val_loss: 0.4081 - val_accuracy: 0.8268\n",
      "Epoch 88/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4655 - accuracy: 0.7949 - val_loss: 0.4100 - val_accuracy: 0.8324\n",
      "Epoch 89/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4753 - accuracy: 0.7949 - val_loss: 0.4089 - val_accuracy: 0.8324\n",
      "Epoch 90/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4801 - accuracy: 0.7978 - val_loss: 0.4094 - val_accuracy: 0.8324\n",
      "Epoch 91/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4668 - accuracy: 0.8020 - val_loss: 0.4121 - val_accuracy: 0.8380\n",
      "Epoch 92/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4682 - accuracy: 0.8034 - val_loss: 0.4112 - val_accuracy: 0.8380\n",
      "Epoch 93/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4636 - accuracy: 0.8118 - val_loss: 0.4124 - val_accuracy: 0.8380\n",
      "Epoch 94/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4716 - accuracy: 0.7978 - val_loss: 0.4109 - val_accuracy: 0.8380\n",
      "Epoch 95/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4688 - accuracy: 0.8048 - val_loss: 0.4094 - val_accuracy: 0.8380\n",
      "Epoch 96/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4798 - accuracy: 0.7893 - val_loss: 0.4081 - val_accuracy: 0.8380\n",
      "Epoch 97/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4572 - accuracy: 0.8006 - val_loss: 0.4056 - val_accuracy: 0.8380\n",
      "Epoch 98/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4871 - accuracy: 0.7949 - val_loss: 0.4058 - val_accuracy: 0.8380\n",
      "Epoch 99/5000\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.4569 - accuracy: 0.8104 - val_loss: 0.4044 - val_accuracy: 0.8436\n",
      "Epoch 100/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4785 - accuracy: 0.8020 - val_loss: 0.4047 - val_accuracy: 0.8380\n",
      "Epoch 101/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4627 - accuracy: 0.8104 - val_loss: 0.4050 - val_accuracy: 0.8380\n",
      "Epoch 102/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4717 - accuracy: 0.8020 - val_loss: 0.4041 - val_accuracy: 0.8436\n",
      "Epoch 103/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4464 - accuracy: 0.8160 - val_loss: 0.4042 - val_accuracy: 0.8380\n",
      "Epoch 104/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4710 - accuracy: 0.7921 - val_loss: 0.4031 - val_accuracy: 0.8436\n",
      "Epoch 105/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4859 - accuracy: 0.8034 - val_loss: 0.4041 - val_accuracy: 0.8492\n",
      "Epoch 106/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4492 - accuracy: 0.8006 - val_loss: 0.4034 - val_accuracy: 0.8492\n",
      "Epoch 107/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4827 - accuracy: 0.8090 - val_loss: 0.4050 - val_accuracy: 0.8492\n",
      "Epoch 108/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4616 - accuracy: 0.8202 - val_loss: 0.4067 - val_accuracy: 0.8436\n",
      "Epoch 109/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4669 - accuracy: 0.8090 - val_loss: 0.4061 - val_accuracy: 0.8380\n",
      "Epoch 110/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4561 - accuracy: 0.7992 - val_loss: 0.4064 - val_accuracy: 0.8380\n",
      "Epoch 111/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4589 - accuracy: 0.8062 - val_loss: 0.4047 - val_accuracy: 0.8380\n",
      "Epoch 112/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4670 - accuracy: 0.8132 - val_loss: 0.4037 - val_accuracy: 0.8324\n",
      "Epoch 113/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4585 - accuracy: 0.8132 - val_loss: 0.3998 - val_accuracy: 0.8380\n",
      "Epoch 114/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4658 - accuracy: 0.7921 - val_loss: 0.4007 - val_accuracy: 0.8380\n",
      "Epoch 115/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4593 - accuracy: 0.7921 - val_loss: 0.4017 - val_accuracy: 0.8380\n",
      "Epoch 116/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4675 - accuracy: 0.8062 - val_loss: 0.4003 - val_accuracy: 0.8380\n",
      "Epoch 117/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4751 - accuracy: 0.8062 - val_loss: 0.4002 - val_accuracy: 0.8324\n",
      "Epoch 118/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4662 - accuracy: 0.8048 - val_loss: 0.4000 - val_accuracy: 0.8380\n",
      "Epoch 119/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4632 - accuracy: 0.8076 - val_loss: 0.4007 - val_accuracy: 0.8436\n",
      "Epoch 120/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4824 - accuracy: 0.7921 - val_loss: 0.4012 - val_accuracy: 0.8436\n",
      "Epoch 121/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4702 - accuracy: 0.7935 - val_loss: 0.4006 - val_accuracy: 0.8492\n",
      "Epoch 122/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4824 - accuracy: 0.7795 - val_loss: 0.4007 - val_accuracy: 0.8492\n",
      "Epoch 123/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4656 - accuracy: 0.7949 - val_loss: 0.4004 - val_accuracy: 0.8492\n",
      "Epoch 124/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4783 - accuracy: 0.7949 - val_loss: 0.4042 - val_accuracy: 0.8436\n",
      "Epoch 125/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4618 - accuracy: 0.8090 - val_loss: 0.4022 - val_accuracy: 0.8436\n",
      "Epoch 126/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4702 - accuracy: 0.7935 - val_loss: 0.4015 - val_accuracy: 0.8380\n",
      "Epoch 127/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4676 - accuracy: 0.8076 - val_loss: 0.4012 - val_accuracy: 0.8380\n",
      "Epoch 128/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4543 - accuracy: 0.8034 - val_loss: 0.4006 - val_accuracy: 0.8436\n",
      "Epoch 129/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4546 - accuracy: 0.7992 - val_loss: 0.3999 - val_accuracy: 0.8436\n",
      "Epoch 130/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4475 - accuracy: 0.8216 - val_loss: 0.3981 - val_accuracy: 0.8380\n",
      "Epoch 131/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4569 - accuracy: 0.8188 - val_loss: 0.3973 - val_accuracy: 0.8380\n",
      "Epoch 132/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4551 - accuracy: 0.8104 - val_loss: 0.3954 - val_accuracy: 0.8380\n",
      "Epoch 133/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4511 - accuracy: 0.8104 - val_loss: 0.3951 - val_accuracy: 0.8380\n",
      "Epoch 134/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4644 - accuracy: 0.7963 - val_loss: 0.3970 - val_accuracy: 0.8380\n",
      "Epoch 135/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4859 - accuracy: 0.7921 - val_loss: 0.3954 - val_accuracy: 0.8324\n",
      "Epoch 136/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4749 - accuracy: 0.7949 - val_loss: 0.3970 - val_accuracy: 0.8380\n",
      "Epoch 137/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4726 - accuracy: 0.7921 - val_loss: 0.3963 - val_accuracy: 0.8380\n",
      "Epoch 138/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4628 - accuracy: 0.8020 - val_loss: 0.3978 - val_accuracy: 0.8380\n",
      "Epoch 139/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4445 - accuracy: 0.8090 - val_loss: 0.3998 - val_accuracy: 0.8380\n",
      "Epoch 140/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4492 - accuracy: 0.8076 - val_loss: 0.3988 - val_accuracy: 0.8380\n",
      "Epoch 141/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4748 - accuracy: 0.8020 - val_loss: 0.3967 - val_accuracy: 0.8436\n",
      "Epoch 142/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4753 - accuracy: 0.8006 - val_loss: 0.3991 - val_accuracy: 0.8436\n",
      "Epoch 143/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4809 - accuracy: 0.7879 - val_loss: 0.3995 - val_accuracy: 0.8380\n",
      "Epoch 144/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4575 - accuracy: 0.8034 - val_loss: 0.3995 - val_accuracy: 0.8324\n",
      "Epoch 145/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4679 - accuracy: 0.8020 - val_loss: 0.4003 - val_accuracy: 0.8436\n",
      "Epoch 146/5000\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.4702 - accuracy: 0.8034 - val_loss: 0.4033 - val_accuracy: 0.8324\n",
      "Epoch 147/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4619 - accuracy: 0.7963 - val_loss: 0.4034 - val_accuracy: 0.8380\n",
      "Epoch 148/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4694 - accuracy: 0.7893 - val_loss: 0.4037 - val_accuracy: 0.8324\n",
      "Epoch 149/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4529 - accuracy: 0.8076 - val_loss: 0.3972 - val_accuracy: 0.8380\n",
      "Epoch 150/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4533 - accuracy: 0.8104 - val_loss: 0.3978 - val_accuracy: 0.8380\n",
      "Epoch 151/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4653 - accuracy: 0.7921 - val_loss: 0.3977 - val_accuracy: 0.8436\n",
      "Epoch 152/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4650 - accuracy: 0.8048 - val_loss: 0.3992 - val_accuracy: 0.8380\n",
      "Epoch 153/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4592 - accuracy: 0.8090 - val_loss: 0.4015 - val_accuracy: 0.8380\n",
      "Epoch 154/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4451 - accuracy: 0.8230 - val_loss: 0.3960 - val_accuracy: 0.8492\n",
      "Epoch 155/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4581 - accuracy: 0.8006 - val_loss: 0.3950 - val_accuracy: 0.8492\n",
      "Epoch 156/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4827 - accuracy: 0.7978 - val_loss: 0.3953 - val_accuracy: 0.8492\n",
      "Epoch 157/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4617 - accuracy: 0.8020 - val_loss: 0.3955 - val_accuracy: 0.8492\n",
      "Epoch 158/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4661 - accuracy: 0.8048 - val_loss: 0.3981 - val_accuracy: 0.8380\n",
      "Epoch 159/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4600 - accuracy: 0.8076 - val_loss: 0.3978 - val_accuracy: 0.8436\n",
      "Epoch 160/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4646 - accuracy: 0.8020 - val_loss: 0.3989 - val_accuracy: 0.8380\n",
      "Epoch 161/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4616 - accuracy: 0.7935 - val_loss: 0.3980 - val_accuracy: 0.8380\n",
      "Epoch 162/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4636 - accuracy: 0.7963 - val_loss: 0.3970 - val_accuracy: 0.8436\n",
      "Epoch 163/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4622 - accuracy: 0.8146 - val_loss: 0.3958 - val_accuracy: 0.8436\n",
      "Epoch 164/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4585 - accuracy: 0.8090 - val_loss: 0.3960 - val_accuracy: 0.8492\n",
      "Epoch 165/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4592 - accuracy: 0.8034 - val_loss: 0.3973 - val_accuracy: 0.8492\n",
      "Epoch 166/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4635 - accuracy: 0.8048 - val_loss: 0.3949 - val_accuracy: 0.8492\n",
      "Epoch 167/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4479 - accuracy: 0.8062 - val_loss: 0.3942 - val_accuracy: 0.8492\n",
      "Epoch 168/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4705 - accuracy: 0.7865 - val_loss: 0.3956 - val_accuracy: 0.8492\n",
      "Epoch 169/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4532 - accuracy: 0.7992 - val_loss: 0.3952 - val_accuracy: 0.8436\n",
      "Epoch 170/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4581 - accuracy: 0.8104 - val_loss: 0.3948 - val_accuracy: 0.8436\n",
      "Epoch 171/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4566 - accuracy: 0.7921 - val_loss: 0.3958 - val_accuracy: 0.8436\n",
      "Epoch 172/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4672 - accuracy: 0.8006 - val_loss: 0.3964 - val_accuracy: 0.8380\n",
      "Epoch 173/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4485 - accuracy: 0.8160 - val_loss: 0.3951 - val_accuracy: 0.8380\n",
      "Epoch 174/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4608 - accuracy: 0.8020 - val_loss: 0.3946 - val_accuracy: 0.8436\n",
      "Epoch 175/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4509 - accuracy: 0.8118 - val_loss: 0.3962 - val_accuracy: 0.8324\n",
      "Epoch 176/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4487 - accuracy: 0.8146 - val_loss: 0.3998 - val_accuracy: 0.8380\n",
      "Epoch 177/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4431 - accuracy: 0.8090 - val_loss: 0.3990 - val_accuracy: 0.8380\n",
      "Epoch 178/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4597 - accuracy: 0.8146 - val_loss: 0.3959 - val_accuracy: 0.8436\n",
      "Epoch 179/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4565 - accuracy: 0.8020 - val_loss: 0.3949 - val_accuracy: 0.8324\n",
      "Epoch 180/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4598 - accuracy: 0.8062 - val_loss: 0.3938 - val_accuracy: 0.8324\n",
      "Epoch 181/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4664 - accuracy: 0.8034 - val_loss: 0.3939 - val_accuracy: 0.8436\n",
      "Epoch 182/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4435 - accuracy: 0.8188 - val_loss: 0.3949 - val_accuracy: 0.8380\n",
      "Epoch 183/5000\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.4692 - accuracy: 0.7963 - val_loss: 0.3947 - val_accuracy: 0.8380\n",
      "Epoch 184/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4585 - accuracy: 0.7963 - val_loss: 0.3937 - val_accuracy: 0.8380\n",
      "Epoch 185/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4562 - accuracy: 0.8034 - val_loss: 0.3909 - val_accuracy: 0.8436\n",
      "Epoch 186/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4481 - accuracy: 0.8090 - val_loss: 0.3898 - val_accuracy: 0.8492\n",
      "Epoch 187/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4526 - accuracy: 0.8062 - val_loss: 0.3931 - val_accuracy: 0.8436\n",
      "Epoch 188/5000\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.4525 - accuracy: 0.7921 - val_loss: 0.3929 - val_accuracy: 0.8436\n",
      "Epoch 189/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4643 - accuracy: 0.8048 - val_loss: 0.3923 - val_accuracy: 0.8492\n",
      "Epoch 190/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4664 - accuracy: 0.8104 - val_loss: 0.3924 - val_accuracy: 0.8436\n",
      "Epoch 191/5000\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.4519 - accuracy: 0.8034 - val_loss: 0.3913 - val_accuracy: 0.8436\n",
      "Epoch 192/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4632 - accuracy: 0.8118 - val_loss: 0.3900 - val_accuracy: 0.8547\n",
      "Epoch 193/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4677 - accuracy: 0.7935 - val_loss: 0.3910 - val_accuracy: 0.8547\n",
      "Epoch 194/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4580 - accuracy: 0.8146 - val_loss: 0.3914 - val_accuracy: 0.8547\n",
      "Epoch 195/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4632 - accuracy: 0.7992 - val_loss: 0.3922 - val_accuracy: 0.8492\n",
      "Epoch 196/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4423 - accuracy: 0.8132 - val_loss: 0.3926 - val_accuracy: 0.8436\n",
      "Epoch 197/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4504 - accuracy: 0.8020 - val_loss: 0.3905 - val_accuracy: 0.8492\n",
      "Epoch 198/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4593 - accuracy: 0.8076 - val_loss: 0.3893 - val_accuracy: 0.8492\n",
      "Epoch 199/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4422 - accuracy: 0.8006 - val_loss: 0.3890 - val_accuracy: 0.8492\n",
      "Epoch 200/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4457 - accuracy: 0.7992 - val_loss: 0.3870 - val_accuracy: 0.8547\n",
      "Epoch 201/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4543 - accuracy: 0.8104 - val_loss: 0.3863 - val_accuracy: 0.8492\n",
      "Epoch 202/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4481 - accuracy: 0.8076 - val_loss: 0.3899 - val_accuracy: 0.8492\n",
      "Epoch 203/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4745 - accuracy: 0.8118 - val_loss: 0.3911 - val_accuracy: 0.8492\n",
      "Epoch 204/5000\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.4560 - accuracy: 0.8104 - val_loss: 0.3909 - val_accuracy: 0.8436\n",
      "Epoch 205/5000\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.4524 - accuracy: 0.8048 - val_loss: 0.3911 - val_accuracy: 0.8436\n",
      "Epoch 206/5000\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.4538 - accuracy: 0.8020 - val_loss: 0.3889 - val_accuracy: 0.8492\n",
      "Epoch 207/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4442 - accuracy: 0.8202 - val_loss: 0.3873 - val_accuracy: 0.8492\n",
      "Epoch 208/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4492 - accuracy: 0.8076 - val_loss: 0.3895 - val_accuracy: 0.8380\n",
      "Epoch 209/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4425 - accuracy: 0.8034 - val_loss: 0.3916 - val_accuracy: 0.8380\n",
      "Epoch 210/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4596 - accuracy: 0.8076 - val_loss: 0.3899 - val_accuracy: 0.8380\n",
      "Epoch 211/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4747 - accuracy: 0.7978 - val_loss: 0.3899 - val_accuracy: 0.8436\n",
      "Epoch 212/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4628 - accuracy: 0.8146 - val_loss: 0.3906 - val_accuracy: 0.8436\n",
      "Epoch 213/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4560 - accuracy: 0.8006 - val_loss: 0.3876 - val_accuracy: 0.8547\n",
      "Epoch 214/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4501 - accuracy: 0.8090 - val_loss: 0.3871 - val_accuracy: 0.8492\n",
      "Epoch 215/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4568 - accuracy: 0.8020 - val_loss: 0.3862 - val_accuracy: 0.8492\n",
      "Epoch 216/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4537 - accuracy: 0.7992 - val_loss: 0.3862 - val_accuracy: 0.8492\n",
      "Epoch 217/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4572 - accuracy: 0.8020 - val_loss: 0.3865 - val_accuracy: 0.8436\n",
      "Epoch 218/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4415 - accuracy: 0.8062 - val_loss: 0.3856 - val_accuracy: 0.8492\n",
      "Epoch 219/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4384 - accuracy: 0.7992 - val_loss: 0.3879 - val_accuracy: 0.8436\n",
      "Epoch 220/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4575 - accuracy: 0.8160 - val_loss: 0.3887 - val_accuracy: 0.8436\n",
      "Epoch 221/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4527 - accuracy: 0.8034 - val_loss: 0.3855 - val_accuracy: 0.8492\n",
      "Epoch 222/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4692 - accuracy: 0.8006 - val_loss: 0.3863 - val_accuracy: 0.8436\n",
      "Epoch 223/5000\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.4619 - accuracy: 0.7907 - val_loss: 0.3865 - val_accuracy: 0.8492\n",
      "Epoch 224/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4533 - accuracy: 0.8076 - val_loss: 0.3872 - val_accuracy: 0.8436\n",
      "Epoch 225/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4523 - accuracy: 0.8034 - val_loss: 0.3854 - val_accuracy: 0.8492\n",
      "Epoch 226/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4360 - accuracy: 0.8090 - val_loss: 0.3868 - val_accuracy: 0.8436\n",
      "Epoch 227/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4623 - accuracy: 0.8006 - val_loss: 0.3855 - val_accuracy: 0.8436\n",
      "Epoch 228/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4563 - accuracy: 0.8062 - val_loss: 0.3847 - val_accuracy: 0.8492\n",
      "Epoch 229/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4420 - accuracy: 0.8258 - val_loss: 0.3846 - val_accuracy: 0.8436\n",
      "Epoch 230/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4684 - accuracy: 0.7879 - val_loss: 0.3857 - val_accuracy: 0.8492\n",
      "Epoch 231/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4363 - accuracy: 0.8034 - val_loss: 0.3887 - val_accuracy: 0.8436\n",
      "Epoch 232/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4425 - accuracy: 0.8090 - val_loss: 0.3880 - val_accuracy: 0.8380\n",
      "Epoch 233/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4457 - accuracy: 0.8118 - val_loss: 0.3863 - val_accuracy: 0.8380\n",
      "Epoch 234/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4509 - accuracy: 0.8076 - val_loss: 0.3854 - val_accuracy: 0.8380\n",
      "Epoch 235/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4735 - accuracy: 0.7963 - val_loss: 0.3852 - val_accuracy: 0.8436\n",
      "Epoch 236/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4526 - accuracy: 0.8048 - val_loss: 0.3851 - val_accuracy: 0.8436\n",
      "Epoch 237/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4655 - accuracy: 0.8090 - val_loss: 0.3869 - val_accuracy: 0.8436\n",
      "Epoch 238/5000\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.4516 - accuracy: 0.8020 - val_loss: 0.3866 - val_accuracy: 0.8436\n",
      "Epoch 239/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4662 - accuracy: 0.8048 - val_loss: 0.3871 - val_accuracy: 0.8436\n",
      "Epoch 240/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4323 - accuracy: 0.8174 - val_loss: 0.3848 - val_accuracy: 0.8436\n",
      "Epoch 241/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4675 - accuracy: 0.8104 - val_loss: 0.3853 - val_accuracy: 0.8492\n",
      "Epoch 242/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4480 - accuracy: 0.8062 - val_loss: 0.3873 - val_accuracy: 0.8436\n",
      "Epoch 243/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4618 - accuracy: 0.8076 - val_loss: 0.3873 - val_accuracy: 0.8324\n",
      "Epoch 244/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4419 - accuracy: 0.8020 - val_loss: 0.3862 - val_accuracy: 0.8436\n",
      "Epoch 245/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4537 - accuracy: 0.8062 - val_loss: 0.3866 - val_accuracy: 0.8380\n",
      "Epoch 246/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4486 - accuracy: 0.8048 - val_loss: 0.3897 - val_accuracy: 0.8380\n",
      "Epoch 247/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4526 - accuracy: 0.8174 - val_loss: 0.3881 - val_accuracy: 0.8324\n",
      "Epoch 248/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4442 - accuracy: 0.8202 - val_loss: 0.3872 - val_accuracy: 0.8380\n",
      "Epoch 249/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4544 - accuracy: 0.8006 - val_loss: 0.3877 - val_accuracy: 0.8324\n",
      "Epoch 250/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4436 - accuracy: 0.8048 - val_loss: 0.3882 - val_accuracy: 0.8324\n",
      "Epoch 251/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4521 - accuracy: 0.8076 - val_loss: 0.3866 - val_accuracy: 0.8380\n",
      "Epoch 252/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4418 - accuracy: 0.8076 - val_loss: 0.3864 - val_accuracy: 0.8492\n",
      "Epoch 253/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4561 - accuracy: 0.8146 - val_loss: 0.3865 - val_accuracy: 0.8492\n",
      "Epoch 254/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4557 - accuracy: 0.8230 - val_loss: 0.3881 - val_accuracy: 0.8380\n",
      "Epoch 255/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4363 - accuracy: 0.8174 - val_loss: 0.3878 - val_accuracy: 0.8380\n",
      "Epoch 256/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4571 - accuracy: 0.8076 - val_loss: 0.3875 - val_accuracy: 0.8380\n",
      "Epoch 257/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4467 - accuracy: 0.8034 - val_loss: 0.3893 - val_accuracy: 0.8324\n",
      "Epoch 258/5000\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.4490 - accuracy: 0.8006 - val_loss: 0.3884 - val_accuracy: 0.8380\n",
      "Epoch 259/5000\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.4369 - accuracy: 0.8160 - val_loss: 0.3859 - val_accuracy: 0.8380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/01/02 15:40:39 47 [INFO] [train] done in 106.124 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00259: early stopping\n"
     ]
    }
   ],
   "source": [
    "runner = Runner(PROJECT_NAME, ModelNN, cv=False)\n",
    "with timer(\"train\", logger):\n",
    "    runner.train(X_train, y_train, params, train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68408285-c282-49ee-bd5b-0cb383d1e503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3846169710159302"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runner.get_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6a868f6-d019-4fad-8f27-d5a66dc29af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/01/02 15:40:53 45 [INFO] [prediction] start.\n",
      "2022-01-02 15:40:53.120827: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022/01/02 15:40:53 47 [INFO] [prediction] done in 0.199 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.90039206, 0.09960796],\n",
       "       [0.7186646 , 0.2813354 ],\n",
       "       [0.86595374, 0.13404633],\n",
       "       [0.8780423 , 0.12195769],\n",
       "       [0.61403733, 0.38596264],\n",
       "       [0.84271026, 0.15728982],\n",
       "       [0.26888797, 0.731112  ],\n",
       "       [0.8151269 , 0.18487306],\n",
       "       [0.39134094, 0.6086591 ],\n",
       "       [0.87308484, 0.1269152 ],\n",
       "       [0.8822608 , 0.11773922],\n",
       "       [0.784472  , 0.21552801],\n",
       "       [0.04182854, 0.9581714 ],\n",
       "       [0.8900869 , 0.10991312],\n",
       "       [0.07577251, 0.9242274 ],\n",
       "       [0.04961208, 0.95038784],\n",
       "       [0.79316103, 0.20683901],\n",
       "       [0.7740442 , 0.22595578],\n",
       "       [0.617314  , 0.38268602],\n",
       "       [0.5332779 , 0.4667221 ],\n",
       "       [0.63727164, 0.3627284 ],\n",
       "       [0.7957919 , 0.20420808],\n",
       "       [0.04263877, 0.9573612 ],\n",
       "       [0.34846494, 0.6515351 ],\n",
       "       [0.13683441, 0.86316556],\n",
       "       [0.8855458 , 0.11445421],\n",
       "       [0.03464452, 0.9653555 ],\n",
       "       [0.7838079 , 0.21619214],\n",
       "       [0.7696658 , 0.23033425],\n",
       "       [0.80208296, 0.197917  ],\n",
       "       [0.87808245, 0.12191759],\n",
       "       [0.80990916, 0.19009088],\n",
       "       [0.75324035, 0.24675968],\n",
       "       [0.7282271 , 0.27177292],\n",
       "       [0.47149193, 0.52850807],\n",
       "       [0.75732565, 0.24267438],\n",
       "       [0.6027034 , 0.39729664],\n",
       "       [0.5152245 , 0.48477557],\n",
       "       [0.8744986 , 0.12550136],\n",
       "       [0.8577606 , 0.14223939],\n",
       "       [0.846248  , 0.15375207],\n",
       "       [0.7086693 , 0.29133072],\n",
       "       [0.89057785, 0.10942218],\n",
       "       [0.07451022, 0.9254898 ],\n",
       "       [0.06990673, 0.9300933 ],\n",
       "       [0.8751167 , 0.12488326],\n",
       "       [0.6631027 , 0.3368974 ],\n",
       "       [0.8930807 , 0.10691933],\n",
       "       [0.07826938, 0.92173064],\n",
       "       [0.7167972 , 0.2832028 ],\n",
       "       [0.64560807, 0.3543919 ],\n",
       "       [0.7555215 , 0.24447852],\n",
       "       [0.08351882, 0.9164812 ],\n",
       "       [0.17794718, 0.8220529 ],\n",
       "       [0.78161293, 0.21838714],\n",
       "       [0.88229775, 0.11770225],\n",
       "       [0.8872639 , 0.11273611],\n",
       "       [0.8752197 , 0.12478025],\n",
       "       [0.8791849 , 0.12081514],\n",
       "       [0.05006269, 0.94993734],\n",
       "       [0.8571159 , 0.14288405],\n",
       "       [0.86831045, 0.13168956],\n",
       "       [0.8599705 , 0.14002955],\n",
       "       [0.20256677, 0.7974332 ],\n",
       "       [0.4798548 , 0.52014524],\n",
       "       [0.07387667, 0.92612326],\n",
       "       [0.18028307, 0.819717  ],\n",
       "       [0.7879834 , 0.21201666],\n",
       "       [0.52314734, 0.47685272],\n",
       "       [0.51615095, 0.48384905],\n",
       "       [0.21877538, 0.7812246 ],\n",
       "       [0.86727744, 0.13272256],\n",
       "       [0.5956962 , 0.40430382],\n",
       "       [0.48725072, 0.5127493 ],\n",
       "       [0.04716077, 0.95283926],\n",
       "       [0.43501085, 0.56498915],\n",
       "       [0.88219994, 0.11780009],\n",
       "       [0.11700124, 0.8829987 ],\n",
       "       [0.8617285 , 0.13827147],\n",
       "       [0.21877538, 0.7812246 ],\n",
       "       [0.69185525, 0.3081447 ],\n",
       "       [0.749979  , 0.250021  ],\n",
       "       [0.7933985 , 0.20660152],\n",
       "       [0.8822608 , 0.11773922],\n",
       "       [0.74698734, 0.25301266],\n",
       "       [0.80052525, 0.1994747 ],\n",
       "       [0.2427154 , 0.7572846 ],\n",
       "       [0.46997982, 0.53002024],\n",
       "       [0.26778316, 0.7322168 ],\n",
       "       [0.3558062 , 0.6441938 ],\n",
       "       [0.57772905, 0.42227092],\n",
       "       [0.88230634, 0.11769361],\n",
       "       [0.05623234, 0.94376767],\n",
       "       [0.88219994, 0.11780009],\n",
       "       [0.446414  , 0.55358607],\n",
       "       [0.8751658 , 0.12483417],\n",
       "       [0.2033093 , 0.7966907 ],\n",
       "       [0.8811402 , 0.11885978],\n",
       "       [0.5021513 , 0.49784866],\n",
       "       [0.8855362 , 0.11446384],\n",
       "       [0.05062715, 0.9493728 ],\n",
       "       [0.8249985 , 0.17500153],\n",
       "       [0.8930807 , 0.10691933],\n",
       "       [0.8768349 , 0.12316507],\n",
       "       [0.36274388, 0.63725615],\n",
       "       [0.87364614, 0.12635382],\n",
       "       [0.8685008 , 0.13149914],\n",
       "       [0.8930807 , 0.10691933],\n",
       "       [0.88193005, 0.11806998],\n",
       "       [0.77812266, 0.22187734],\n",
       "       [0.83506525, 0.16493477],\n",
       "       [0.26779515, 0.7322049 ],\n",
       "       [0.04758655, 0.95241344],\n",
       "       [0.18271524, 0.8172848 ],\n",
       "       [0.19313547, 0.8068645 ],\n",
       "       [0.77212876, 0.22787128],\n",
       "       [0.8189513 , 0.18104877],\n",
       "       [0.2601249 , 0.73987514],\n",
       "       [0.5642597 , 0.43574032],\n",
       "       [0.07815515, 0.92184484],\n",
       "       [0.05145105, 0.9485489 ],\n",
       "       [0.88298947, 0.11701061],\n",
       "       [0.04291573, 0.95708424],\n",
       "       [0.88005495, 0.11994512],\n",
       "       [0.8930807 , 0.10691933],\n",
       "       [0.4052873 , 0.5947127 ],\n",
       "       [0.86947215, 0.13052788],\n",
       "       [0.34684527, 0.6531547 ],\n",
       "       [0.8924296 , 0.10757042],\n",
       "       [0.8732847 , 0.12671532],\n",
       "       [0.884566  , 0.11543398],\n",
       "       [0.702854  , 0.29714602],\n",
       "       [0.66751474, 0.33248526],\n",
       "       [0.80684304, 0.19315694],\n",
       "       [0.8916083 , 0.10839175],\n",
       "       [0.8733585 , 0.12664151],\n",
       "       [0.8076701 , 0.19232988],\n",
       "       [0.8453637 , 0.1546363 ],\n",
       "       [0.53944266, 0.46055734],\n",
       "       [0.9279102 , 0.0720898 ],\n",
       "       [0.8413356 , 0.1586644 ],\n",
       "       [0.04975604, 0.95024395],\n",
       "       [0.7807426 , 0.21925747],\n",
       "       [0.8504513 , 0.14954877],\n",
       "       [0.7734886 , 0.22651145],\n",
       "       [0.89688784, 0.10311212],\n",
       "       [0.70547163, 0.29452837],\n",
       "       [0.86937255, 0.1306274 ],\n",
       "       [0.7086693 , 0.29133072],\n",
       "       [0.8419195 , 0.15808052],\n",
       "       [0.03515838, 0.9648416 ],\n",
       "       [0.817806  , 0.182194  ],\n",
       "       [0.9013968 , 0.09860319],\n",
       "       [0.7182534 , 0.28174666],\n",
       "       [0.89860666, 0.10139336],\n",
       "       [0.8734763 , 0.1265237 ],\n",
       "       [0.05333879, 0.9466612 ],\n",
       "       [0.53959453, 0.4604054 ],\n",
       "       [0.7734886 , 0.22651145],\n",
       "       [0.5629036 , 0.43709645],\n",
       "       [0.26777127, 0.73222876],\n",
       "       [0.7004118 , 0.29958823],\n",
       "       [0.06662281, 0.9333772 ],\n",
       "       [0.8826143 , 0.11738571],\n",
       "       [0.8905718 , 0.10942822],\n",
       "       [0.6597572 , 0.34024286],\n",
       "       [0.6841717 , 0.31582835],\n",
       "       [0.87725705, 0.12274302],\n",
       "       [0.04336427, 0.9566358 ],\n",
       "       [0.5273858 , 0.47261432],\n",
       "       [0.88239753, 0.1176025 ],\n",
       "       [0.8097512 , 0.1902488 ],\n",
       "       [0.8686699 , 0.13133009],\n",
       "       [0.81832033, 0.18167976],\n",
       "       [0.9243424 , 0.07565762],\n",
       "       [0.0532109 , 0.9467891 ],\n",
       "       [0.05935713, 0.9406429 ],\n",
       "       [0.6333548 , 0.3666452 ],\n",
       "       [0.15081906, 0.8491809 ],\n",
       "       [0.09409407, 0.9059059 ],\n",
       "       [0.8617285 , 0.13827147],\n",
       "       [0.64104146, 0.3589586 ],\n",
       "       [0.03786461, 0.9621354 ],\n",
       "       [0.8930807 , 0.10691933],\n",
       "       [0.05157333, 0.94842666],\n",
       "       [0.8878028 , 0.11219724],\n",
       "       [0.05803768, 0.94196236],\n",
       "       [0.8682796 , 0.13172038],\n",
       "       [0.90920067, 0.09079935],\n",
       "       [0.8886151 , 0.11138497],\n",
       "       [0.8530479 , 0.14695211],\n",
       "       [0.70871496, 0.29128504],\n",
       "       [0.83157057, 0.1684294 ],\n",
       "       [0.8644434 , 0.13555664],\n",
       "       [0.39110893, 0.60889107],\n",
       "       [0.8856163 , 0.11438375],\n",
       "       [0.18693566, 0.8130644 ],\n",
       "       [0.4700414 , 0.52995855],\n",
       "       [0.8277375 , 0.17226255],\n",
       "       [0.60264134, 0.39735863],\n",
       "       [0.27157703, 0.728423  ],\n",
       "       [0.6959943 , 0.3040057 ],\n",
       "       [0.5311578 , 0.46884224],\n",
       "       [0.04537519, 0.9546248 ],\n",
       "       [0.8408251 , 0.159175  ],\n",
       "       [0.5077398 , 0.4922603 ],\n",
       "       [0.33511353, 0.6648865 ],\n",
       "       [0.8353097 , 0.16469032],\n",
       "       [0.0454156 , 0.95458436],\n",
       "       [0.87512654, 0.12487342],\n",
       "       [0.8781121 , 0.12188796],\n",
       "       [0.8825942 , 0.11740581],\n",
       "       [0.7226533 , 0.27734673],\n",
       "       [0.2757421 , 0.72425795],\n",
       "       [0.87928927, 0.12071076],\n",
       "       [0.772099  , 0.227901  ],\n",
       "       [0.2678429 , 0.7321571 ],\n",
       "       [0.8108758 , 0.18912424],\n",
       "       [0.07962174, 0.92037827],\n",
       "       [0.88219994, 0.11780009],\n",
       "       [0.05668839, 0.94331163],\n",
       "       [0.86721706, 0.13278294],\n",
       "       [0.06173483, 0.9382652 ],\n",
       "       [0.8673177 , 0.13268234],\n",
       "       [0.0714359 , 0.9285641 ],\n",
       "       [0.46395975, 0.5360403 ],\n",
       "       [0.87149316, 0.12850681],\n",
       "       [0.26778316, 0.7322168 ],\n",
       "       [0.88691026, 0.11308983],\n",
       "       [0.8796557 , 0.12034434],\n",
       "       [0.6775162 , 0.3224838 ],\n",
       "       [0.03808266, 0.96191734],\n",
       "       [0.86648667, 0.13351338],\n",
       "       [0.8930372 , 0.1069628 ],\n",
       "       [0.5654016 , 0.43459842],\n",
       "       [0.86499506, 0.1350049 ],\n",
       "       [0.67185885, 0.32814124],\n",
       "       [0.76710576, 0.23289433],\n",
       "       [0.05858983, 0.94141024],\n",
       "       [0.05640957, 0.94359046],\n",
       "       [0.07542591, 0.9245741 ],\n",
       "       [0.19697692, 0.8030231 ],\n",
       "       [0.67124146, 0.32875854],\n",
       "       [0.8822659 , 0.11773413],\n",
       "       [0.89307106, 0.10692898],\n",
       "       [0.7541496 , 0.24585044],\n",
       "       [0.06143901, 0.93856096],\n",
       "       [0.88822275, 0.11177732],\n",
       "       [0.07815515, 0.92184484],\n",
       "       [0.4635731 , 0.5364269 ],\n",
       "       [0.03981705, 0.96018296],\n",
       "       [0.8649697 , 0.13503037],\n",
       "       [0.44214648, 0.5578535 ],\n",
       "       [0.87278473, 0.12721528],\n",
       "       [0.8844498 , 0.11555025],\n",
       "       [0.88239753, 0.1176025 ],\n",
       "       [0.8930807 , 0.10691933],\n",
       "       [0.8797374 , 0.12026261],\n",
       "       [0.05841097, 0.941589  ],\n",
       "       [0.8673227 , 0.13267733],\n",
       "       [0.8846606 , 0.11533941],\n",
       "       [0.8672926 , 0.13270748],\n",
       "       [0.08224384, 0.9177562 ],\n",
       "       [0.25723404, 0.74276596],\n",
       "       [0.78418344, 0.21581654],\n",
       "       [0.8822608 , 0.11773922],\n",
       "       [0.70577216, 0.29422787],\n",
       "       [0.88239753, 0.1176025 ],\n",
       "       [0.6027034 , 0.39729664],\n",
       "       [0.85678405, 0.14321592],\n",
       "       [0.6696889 , 0.3303111 ],\n",
       "       [0.8930807 , 0.10691933],\n",
       "       [0.03790165, 0.9620984 ],\n",
       "       [0.27733088, 0.7226691 ],\n",
       "       [0.81832033, 0.18167976],\n",
       "       [0.0598904 , 0.94010955],\n",
       "       [0.8547405 , 0.14525948],\n",
       "       [0.86675644, 0.13324356],\n",
       "       [0.8399092 , 0.16009085],\n",
       "       [0.81942266, 0.18057737],\n",
       "       [0.5389429 , 0.46105716],\n",
       "       [0.75969493, 0.2403051 ],\n",
       "       [0.26778316, 0.7322168 ],\n",
       "       [0.29806137, 0.7019386 ],\n",
       "       [0.27656433, 0.72343564],\n",
       "       [0.8882521 , 0.11174791],\n",
       "       [0.88252366, 0.11747634],\n",
       "       [0.64606124, 0.35393873],\n",
       "       [0.81832033, 0.18167976],\n",
       "       [0.88219994, 0.11780009],\n",
       "       [0.7079989 , 0.2920011 ],\n",
       "       [0.26856893, 0.7314311 ],\n",
       "       [0.81832033, 0.18167976],\n",
       "       [0.8399035 , 0.16009656],\n",
       "       [0.88722146, 0.11277857],\n",
       "       [0.8767907 , 0.1232093 ],\n",
       "       [0.03460107, 0.9653989 ],\n",
       "       [0.80208296, 0.197917  ],\n",
       "       [0.7083884 , 0.2916116 ],\n",
       "       [0.88116527, 0.11883467],\n",
       "       [0.88467604, 0.11532395],\n",
       "       [0.78192407, 0.21807596],\n",
       "       [0.89755136, 0.10244869],\n",
       "       [0.87304276, 0.12695721],\n",
       "       [0.26778316, 0.7322168 ],\n",
       "       [0.14840016, 0.8515999 ],\n",
       "       [0.7706198 , 0.22938025],\n",
       "       [0.7177308 , 0.28226924],\n",
       "       [0.8394239 , 0.16057613],\n",
       "       [0.70859253, 0.29140753],\n",
       "       [0.8595884 , 0.14041162],\n",
       "       [0.7806372 , 0.21936281],\n",
       "       [0.8823874 , 0.11761262],\n",
       "       [0.37920925, 0.6207907 ],\n",
       "       [0.06518221, 0.9348178 ],\n",
       "       [0.17499395, 0.82500607],\n",
       "       [0.6348639 , 0.36513606],\n",
       "       [0.78489345, 0.21510662],\n",
       "       [0.8783594 , 0.12164066],\n",
       "       [0.7979244 , 0.20207559],\n",
       "       [0.8768349 , 0.12316507],\n",
       "       [0.79905206, 0.20094793],\n",
       "       [0.8453637 , 0.1546363 ],\n",
       "       [0.7295076 , 0.2704924 ],\n",
       "       [0.06425342, 0.9357466 ],\n",
       "       [0.87177855, 0.12822144],\n",
       "       [0.06335109, 0.9366489 ],\n",
       "       [0.6687837 , 0.33121634],\n",
       "       [0.8346877 , 0.16531235],\n",
       "       [0.809042  , 0.19095808],\n",
       "       [0.22746015, 0.7725398 ],\n",
       "       [0.60728693, 0.39271307],\n",
       "       [0.81832033, 0.18167976],\n",
       "       [0.39973682, 0.6002632 ],\n",
       "       [0.8783446 , 0.12165546],\n",
       "       [0.70987886, 0.29012114],\n",
       "       [0.868438  , 0.13156201],\n",
       "       [0.8662662 , 0.1337338 ],\n",
       "       [0.83475274, 0.16524729],\n",
       "       [0.81832033, 0.18167976],\n",
       "       [0.7721179 , 0.2278821 ],\n",
       "       [0.88475966, 0.11524034],\n",
       "       [0.920084  , 0.07991607],\n",
       "       [0.09030973, 0.9096903 ],\n",
       "       [0.8884071 , 0.1115929 ],\n",
       "       [0.43989822, 0.5601018 ],\n",
       "       [0.8453637 , 0.1546363 ],\n",
       "       [0.49638578, 0.5036142 ],\n",
       "       [0.8346808 , 0.16531917],\n",
       "       [0.07644449, 0.92355555],\n",
       "       [0.04852208, 0.9514779 ],\n",
       "       [0.8408251 , 0.159175  ],\n",
       "       [0.737847  , 0.26215303],\n",
       "       [0.9126793 , 0.08732066],\n",
       "       [0.2411472 , 0.75885284],\n",
       "       [0.7962283 , 0.20377176],\n",
       "       [0.14103477, 0.8589652 ],\n",
       "       [0.8822659 , 0.11773413],\n",
       "       [0.8930807 , 0.10691933],\n",
       "       [0.6351453 , 0.3648547 ],\n",
       "       [0.93765634, 0.06234375],\n",
       "       [0.04805962, 0.95194036],\n",
       "       [0.07644449, 0.92355555],\n",
       "       [0.8780423 , 0.12195769],\n",
       "       [0.03655742, 0.9634426 ],\n",
       "       [0.77256215, 0.22743781],\n",
       "       [0.8005151 , 0.19948494],\n",
       "       [0.5007894 , 0.49921066],\n",
       "       [0.04844429, 0.9515557 ],\n",
       "       [0.77514267, 0.22485736],\n",
       "       [0.78444123, 0.21555881],\n",
       "       [0.04197058, 0.9580294 ],\n",
       "       [0.79494935, 0.20505068],\n",
       "       [0.89597625, 0.10402373],\n",
       "       [0.1162199 , 0.88378006],\n",
       "       [0.05762872, 0.94237125],\n",
       "       [0.6736645 , 0.32633546],\n",
       "       [0.8094425 , 0.19055754],\n",
       "       [0.810282  , 0.18971801],\n",
       "       [0.8850794 , 0.11492058],\n",
       "       [0.8930807 , 0.10691933],\n",
       "       [0.8849254 , 0.11507464],\n",
       "       [0.59624606, 0.403754  ],\n",
       "       [0.5504969 , 0.44950312],\n",
       "       [0.86133456, 0.13866542],\n",
       "       [0.09355543, 0.9064446 ],\n",
       "       [0.87338793, 0.12661202],\n",
       "       [0.9020829 , 0.09791706],\n",
       "       [0.8685256 , 0.13147439],\n",
       "       [0.86495185, 0.13504823],\n",
       "       [0.55509347, 0.44490653],\n",
       "       [0.07607941, 0.92392063],\n",
       "       [0.81667477, 0.18332526],\n",
       "       [0.9006377 , 0.09936228],\n",
       "       [0.9041606 , 0.09583939],\n",
       "       [0.03785724, 0.96214277],\n",
       "       [0.8791193 , 0.12088074],\n",
       "       [0.05930653, 0.9406935 ],\n",
       "       [0.8694771 , 0.1305229 ],\n",
       "       [0.89501923, 0.10498077],\n",
       "       [0.04813277, 0.9518672 ],\n",
       "       [0.8642499 , 0.13575014],\n",
       "       [0.03462902, 0.96537095],\n",
       "       [0.45410928, 0.54589075],\n",
       "       [0.57577866, 0.4242214 ],\n",
       "       [0.668955  , 0.33104503],\n",
       "       [0.80131495, 0.19868504],\n",
       "       [0.64190984, 0.35809016],\n",
       "       [0.26776528, 0.7322348 ],\n",
       "       [0.28702968, 0.7129704 ],\n",
       "       [0.26778316, 0.7322168 ],\n",
       "       [0.02982589, 0.9701741 ],\n",
       "       [0.58416176, 0.4158383 ],\n",
       "       [0.88219994, 0.11780009],\n",
       "       [0.04682789, 0.953172  ],\n",
       "       [0.88958305, 0.11041694],\n",
       "       [0.88219994, 0.11780011],\n",
       "       [0.8195104 , 0.18048961]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with timer(\"prediction\", logger):\n",
    "    prob = runner.predict(X_test)\n",
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff953329-92c9-4fa0-83cb-b6a18ff5bf93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n",
       "       1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "       0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = prob.argmax(axis=1)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d25ca66a-ee11-42b0-837e-fce68f562b78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>896</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1304</th>\n",
       "      <td>1305</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305</th>\n",
       "      <td>1306</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306</th>\n",
       "      <td>1307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>1308</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308</th>\n",
       "      <td>1309</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      PassengerId  Survived\n",
       "891           892         0\n",
       "892           893         0\n",
       "893           894         0\n",
       "894           895         0\n",
       "895           896         0\n",
       "...           ...       ...\n",
       "1304         1305         0\n",
       "1305         1306         1\n",
       "1306         1307         0\n",
       "1307         1308         0\n",
       "1308         1309         0\n",
       "\n",
       "[418 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame({\n",
    "    \"PassengerId\": df_test[\"PassengerId\"],\n",
    "    \"Survived\": pred\n",
    "})\n",
    "submission\n",
    "submission.to_csv(f\"../submission/submission_{PROJECT_NAME}.csv\", index=False)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7672fdb-2ad7-48e7-a522-30dbe600255b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 2.77k/2.77k [00:02<00:00, 1.03kB/s]\n",
      "Successfully submitted to Titanic - Machine Learning from Disaster"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit titanic -f ../submission/submission_{PROJECT_NAME}.csv -m \"val_accuracy: 0.8380\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
